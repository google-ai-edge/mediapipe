/* Copyright 2023 The MediaPipe Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

syntax = "proto3";

package mediapipe.tasks.genai.llminference.jni;

option java_package = "com.google.mediapipe.tasks.genai.llminference.jni.proto";
option java_outer_classname = "LlmOptionsProto";

// Configurable parameters for creating an LLM session.
message LlmSessionConfig {
  // Top K number of tokens to be sampled from for each decoding step.
  uint32 topk = 1;

  // Maximum cumulative probability over the tokens to sample from in each
  // decoding step for top-p / nucleus sampling.
  float topp = 2;

  // Randomness when decoding the next token, 0.0f means greedy decoding.
  float temperature = 3;

  // Random seed for sampling tokens.
  optional uint32 random_seed = 4;

  // The absolute path to the LoRA model asset bundle stored locally on the
  // device. This is only compatible with GPU models.
  optional string lora_path = 5;

  // Parameters to customize the graph.
  message GraphConfig {
    // Whether to configure the graph to include the token cost calculator,
    // which allows users to only compute the cost of a prompt.
    optional bool include_token_cost_calculator = 1;

    // Whether to configure the graph to include the vision modality. Only one
    // of enable_vision_modality or enable_audio_modality can be true currently.
    optional bool enable_vision_modality = 2;
  }

  // Parameters to customize the graph.
  optional GraphConfig graph_config = 6;
}

// Configurable model parameters for creating an LLM inference engine.
message LlmModelSettings {
  // Path to the tflite model file.
  string model_path = 1;

  // Directory for saving the weight cache file. If this is not set, the
  // directory provided through model_path will be used for caching purposes.
  string cache_dir = 2;

  // The number of input tokens to process at a time for batch processing
  uint32 sequence_batch_size = 3;

  // The number of decoding steps to run for each GPU-CPU sync. 1 stands for
  // full streaming mode (i.e. the model outputs one token at a time). -1 stands
  // for non-streaming mode (i.e. the model decodes all the way to the end and
  // output the result at once). Note that the more frequent to perform GPU-CPU
  // sync (i.e. closer to 1), the more latency we expect to introduce.
  uint32 num_decode_steps_per_sync = 4;

  // The total number of tokens for kv-cache. In other words, this is the total
  // number of input + output tokens the model needs to handle.
  uint32 max_tokens = 5;

  // Number of supported lora ranks for the base model. Used by GPU only.
  uint32 number_of_supported_lora_ranks = 6;

  // The supported lora ranks for the base model. Used by GPU only.
  repeated uint32 supported_lora_ranks = 7;

  // Maximum top k, which is the max Top-K value supported for all
  // sessions created with the engine, used by GPU only. If a session with Top-K
  // value larger than this is being asked to be created, it will be
  // rejected(throw error). If not provided, the max top k will be 1, which
  // means only greedy decoding is supported for any sessions created with this
  // engine.
  uint32 max_top_k = 8;

  // A container for vision model related settings.
  message VisionModelSettings {
    // Path to the vision encoder model file.
    optional string encoder_path = 1;

    // Path to the vision adapter model file.
    optional string adapter_path = 2;
  }

  optional VisionModelSettings vision_model_settings = 9;

  // Reserved for audio model settings.
  reserved 10;

  // Optional setting to set the preferred backend.
  enum LlmPreferredBackend {
    DEFAULT = 0;
    CPU = 1;
    GPU = 2;
  }
  optional LlmPreferredBackend llm_preferred_backend = 11;
}
